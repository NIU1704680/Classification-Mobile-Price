# -*- coding: utf-8 -*-
"""Classification_Mobile_Price.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ttWyCSZazAR4RGoJE3xiK7sF_u92r05t

## NOM: Joan Aguilar Vilalta                   
## NIU: 1704680

# Mobile Price Classification

## 1. Objetivo del proyecto:

El objetivo de este proyecto es crear un clasificador de precios de teléfonos móviles en función de sus diferentes características técnicas (RAM, batería, cámara, almacenamiento, etc.). Usaremos el dataset Mobile Price Classification de Kaggle, el cual contiene múltiples atributos de hardware y una etiqueta que clasifica cada móvil en una de cuatro categorías de precio:

Kaggle: https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification

GITHUB: https://github.com/NIU1704680/Classification-Mobile-Price.git

## 2. Dataset:

El dataset contiene un total de 21 atributos, explicados a continuacion:

- ID: ID del producto (valor entero)

- Battery_power: Capacidad total de energía que puede almacenar la batería (mAh) (valor entero)

- Blue: Si tiene Bluetooth (valor booleano: 0 = no, 1 = sí)

- Clock_speed: Velocidad del microprocesador en GHz (valor decimal)

- Dual_sim: Si tiene soporte para doble SIM (valor booleano: 0 = no, 1 = sí)

- Fc: Resolución de la cámara frontal en megapíxeles (valor entero)

- Fourth_gen: Si soporta 4G (valor booleano: 0 = no, 1 = sí)

- Int_memory: Memoria interna en GB (valor entero)

- M_dep: Profundidad del móvil en centímetros (valor decimal)

- Mobile_wt: Peso del móvil en gramos (valor entero)

- N_cores: Número de núcleos del procesador (valor entero)

- Pc: Resolución de la cámara posterior en megapíxeles (valor entero)

- Px_height: Altura de la resolución de pantalla en píxeles (valor entero)

- Px_width: Anchura de la resolución de pantalla en píxeles (valor entero)

- Ram: Memoria RAM del dispositivo en MB (valor entero)

- Sc_h: Altura de la pantalla en pulgadas (valor entero)

- Sc_w: Anchura de la pantalla en pulgadas (valor entero)

- Talk_time: Autonomía máxima de conversación (horas) (valor entero)

- Three_g: Si soporta 3G (valor booleano: 0 = no, 1 = sí)

- Touch_screen: Si tiene pantalla táctil (valor booleano: 0 = no, 1 = sí)

- Wifi: Si tiene conexión WiFi (valor booleano: 0 = no, 1 = sí)

Por ultimo encontramos nuestro atributo objetivo (target), el cual el modelo tendrá que predecir:

- Price_range: categoría de precio (0 = baix, 1 = mitjà, 2 = alt, 3 = molt alt)

## 3. Librerías utilizadas:
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import label_binarize
from sklearn.metrics import precision_recall_curve, auc, roc_curve
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split
from sklearn.model_selection import GridSearchCV

"""## 4. Análisis del dataset:

Antes de entrenar modelos de clasificación es necesario comprender la estructura, tipos de datos y valores que contiene el dataset.

Cargamos los datasets:
"""

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

"""Primero miraremos que columnas tenemos y cuantas (utilizado para la explicación anterior del dataset). Para ello usaremos el len():"""

print("Nombre de atributos:", len(train.columns))
print("Nombre de atributos:", train.columns)

"""Necesitamos hacer un análisis inicial de Nans para saber si será necesario hacer un tratamiento o no. No sabemos si el dataset actual contiene Nans, por lo tanto usaremos info() que nos dirá cuántos valores rellenados tiene cada columna:"""

print(train.info())

"""Cada columna contiene exactamente 2000 valores, por lo tanto no existen valores nulos y no es necesario realizar el tratamiento de Nans.

Ahora para saber el tipo de valores que contiene cada una de las columnas utilizamos .dtypes y head():
"""

print(train.dtypes)
print(train.head())

"""Para saber si es necesario normalizar los datos necesitamos saber el rango de valores que tiene cada columna. Si hay rangos muy diferentes necesitaremos hacer esta normalizacion:"""

print(train.describe())

"""Se pueden observar rangos muy diferentes entre variables (por ejemplo, ram vs mobile_wt), por lo que será necesario normalizar.

La siguiente gráfica permite visualizar la media de cada atributo numérico:
"""

# Calcular medias de todos los atributos excepto la clase
means = train.drop(columns=["price_range"]).mean()

# Gráfico
plt.figure(figsize=(14,6))
means.plot(kind="bar")
plt.title("Media de cada atributo del dataset")
plt.xlabel("Atributos")
plt.ylabel("Valor medio")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

"""Con el objetivo de identificar qué atributos influyen más en el precio del dispositivo, calculamos la correlación entre la variable price_range y el resto de características del dataset."""

# Correlación de todas las columnas con price_range
correlation_with_target = train.corr()['price_range'].sort_values(ascending=False)

print(correlation_with_target)

plt.figure(figsize=(8, 10))
sns.barplot(x=correlation_with_target.values, y=correlation_with_target.index)
plt.title("Correlación de cada característica con price_range")
plt.xlabel("Correlación")
plt.ylabel("Características")
plt.show()

"""Como se puede observar, el atributo que presenta la mayor relación con la variable price_range es la memoria RAM, mostrando una correlación significativamente superior al resto. Esto significa que hay una relación fuerte entre estas dos variables, incluso puede llegar a ser una relación lineal.

## 5. Tratamiento de datos:

Comenzaremos separando el atributo objetivo del resto de columnas:
"""

X = train.drop(['price_range'], axis=1)
y = train['price_range']

"""Para ver como normalizar los datos haremos una prueba con tres tipos de normalizacion:"""

#Comparación de diferentes técnicas de normalización y estandarización

# Variables numericas con rangos muy diferentes
data = train[['ram', 'mobile_wt']]

# Escaladores
scalers = {
    "Original": data,
    "StandardScaler": pd.DataFrame(StandardScaler().fit_transform(data), columns=data.columns),
    "MinMaxScaler": pd.DataFrame(MinMaxScaler().fit_transform(data), columns=data.columns),
    "RobustScaler": pd.DataFrame(RobustScaler().fit_transform(data), columns=data.columns),
}

# Visualización comparativa
plt.figure(figsize=(12, 6))

for i, (name, scaled) in enumerate(scalers.items(), 1):
    plt.subplot(2, 2, i)
    sns.kdeplot(scaled['ram'], fill=True, label='ram', alpha=0.5)
    sns.kdeplot(scaled['mobile_wt'], fill=True, label='mobile_wt', alpha=0.5)
    plt.title(name)
    plt.legend()

plt.tight_layout()
plt.show()

"""Aunque StandardScaler, MinMaxScaler y RobustScaler muestran resultados similares, elegimos RobustScaler por su superioridad técnica:

- Utiliza la mediana y el rango intercuartílico (IQR) para centrar y escalar los datos.

- Esto lo hace inherentemente más robusto ante la presencia de outliers (valores atípicos) en las variables, como ram o mobile_wt, asegurando una preparación de datos más fiable.

### Normalizamos los datos:
"""

# Dividir los datos antes de escalar para evitar Data Leakage
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Aplicar RobustScaler
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train) # Ajustar el escalador solo con el conjunto de entrenamiento (el fit sirve para que aprenda parámetros de los datos de train)
X_test_scaled = scaler.transform(X_test) # Aplicar la misma transformación al conjunto de prueba (Transform solamente)

"""## 6. Primer modelo: Logistic Regression

La regresión logística es un modelo simple, eficiente y muy adecuado cuando la relación entre variables y clases es aproximadamente lineal. Se evalúa con validación cruzada estratificada:
"""

# Cross-validation simple

folds = [3, 5, 7, 10]
model = LogisticRegression(max_iter=1000)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')

for k in folds:
    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')

    print(f"{k} folds:")
    print(f"  Accuracy por fold: {scores}")
    print(f"  Accuracy media: {scores.mean():.4f}\n")

# Entrenamos modelo final
model.fit(X_train_scaled, y_train)
test_acc = model.score(X_test_scaled, y_test)

print("Accuracy final en test:", test_acc)

"""Como podemos observar, nuestro primer modelo de regresión logística obtiene una precisión media en el conjunto de entrenamiento de aproximadamente 92.8% según la validación cruzada. Al evaluar el modelo final en el conjunto de prueba, la precisión alcanza un 94.7%, lo que indica que el modelo generaliza muy bien y no muestra signos de memorización (overfitting).

También podemos observar que utilizar 5 folds ofrece un buen equilibrio entre obtener una estimación estable de la accuracy y no incrementar en exceso el número de particiones del conjunto de entrenamiento.

### Analizaremos este resultado calculando la recta ROC, PR y la matriz de confusion:
"""

# Convertimos y a formato binario (One-hot)
y_bin = label_binarize(y_test, classes=[0,1,2,3])
n_classes = y_bin.shape[1]

# Predicciones de probabilidades
y_score = model.predict_proba(X_test_scaled)  # lista de arrays por clase

# Para cada clase, dibujamos la curva Precision-Recall
plt.figure(figsize=(8,6))
for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(y_bin[:, i], y_score[:, i])
    pr_auc = auc(recall, precision)
    plt.plot(recall, precision, label=f'Clase {i} (AUC={pr_auc:.2f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Curvas Precision-Recall (Multiclase)')
plt.legend()
plt.show()

# Binarizamos las clases
y_bin = label_binarize(y_test, classes=[0,1,2,3])
n_classes = y_bin.shape[1]

# Probabilidades predichas por el modelo
y_score = model.predict_proba(X_test_scaled)

# Dibujar ROC para cada clase
plt.figure(figsize=(8,6))
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_bin[:, i], y_score[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Clase {i} (AUC={roc_auc:.2f})')

plt.plot([0,1], [0,1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Curvas ROC (Multiclase)')
plt.legend()
plt.show()

# Usar el modelo entrenado y los datos de prueba correctos
disp = ConfusionMatrixDisplay.from_estimator(
    model,
    X_test_scaled, # Usar datos de prueba ESCALADOS
    y_test,        # Usar etiquetas de prueba reales
    cmap=plt.cm.Blues,
    normalize=None
)

# Corregir el título a Multiclase
disp.ax_.set_title("Matriz de Confusión - Regressión Logística (Multiclase)")
plt.show()

"""El modelo funciona especialmente bien clasificando las clases 0 y 3, mientras que presenta algo más de confusión entre las clases 1 y 2.

El classification_report confirma este comportamiento.
"""

# Generar predicciones de clase
y_pred_test = model.predict(X_test_scaled)

report_log = classification_report(y_test, y_pred_test)
print(report_log)

"""## 7. Segundo modelo: RandomForest

Para mejorar la eficiencia de RandomForest utilizaremos GridSearchCV para la búsqueda de los mejores hiperparámetros. De esta forma conseguimos aumentar el rendimiento
"""

# Definir modelo base
rf = RandomForestClassifier(random_state=42)

# Definir hiperparámetros a probar
param_grid = {
    'n_estimators': [100, 200, 300, 400, 500, 700, 1000],
    'max_depth': [5, 10, 15, 20, 30, 40, None],
    'max_features': ['sqrt', 'log2']
}

# GridSearchCV con validación cruzada 5-fold
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, scoring='accuracy', n_jobs=-1)

# Entrenar
grid_search.fit(X_train_scaled, y_train)

# Mejor modelo
print("Mejores hiperparámetros:", grid_search.best_params_)

"""Como hemos visto la mejor combinación es: 'max_depth': 15, 'max_features': 'sqrt', 'n_estimators': 300"""

# Número de folds que queremos probar
folds = [3, 5, 7, 10]

# Modelo Random Forest
model = model = RandomForestClassifier(n_estimators=300,max_depth=15,max_features='sqrt',random_state=42)

# Cross-validation para cada número de folds
for k in folds:
    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')

    print(f"{k} folds:")
    print(f"  Accuracy por fold: {scores}")
    print(f"  Accuracy media: {scores.mean():.4f}\n")

# Entrenamos modelo final sobre todo el set de entrenamiento
model.fit(X_train_scaled, y_train)
test_acc = model.score(X_test_scaled, y_test)

print("Accuracy final en test:", test_acc)

"""Como podemos observar, con 3 folds el modelo obtiene una precisión media de 85.5%, mientras que al aumentar a 5, 7 y 10 folds la precisión media mejora ligeramente, alcanzando aproximadamente 86.2–87.3%. La evaluación final en el conjunto de prueba muestra una precisión de 87.66%, lo que indica que el modelo generaliza correctamente y no presenta sobreajuste.

Además, utilizar un número intermedio de folds, como 5 o 7, ofrece un buen equilibrio entre obtener una estimación estable de la accuracy y no incrementar demasiado el número de particiones del conjunto de entrenamiento.

### Analizaremos este resultado calculando la recta ROC, PR y la matriz de confusion:
"""

# Convertimos y a formato binario (One-hot)
y_bin = label_binarize(y_test, classes=[0,1,2,3])
n_classes = y_bin.shape[1]

# Predicciones de probabilidades
y_score = model.predict_proba(X_test_scaled)  # lista de arrays por clase

# Para cada clase, dibujamos la curva Precision-Recall
plt.figure(figsize=(8,6))
for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(y_bin[:, i], y_score[:, i])
    pr_auc = auc(recall, precision)
    plt.plot(recall, precision, label=f'Clase {i} (AUC={pr_auc:.2f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Curvas Precision-Recall (Multiclase)')
plt.legend()
plt.show()

# Binarizamos las clases
y_bin = label_binarize(y_test, classes=[0,1,2,3])
n_classes = y_bin.shape[1]

# Probabilidades predichas por el modelo
y_score = model.predict_proba(X_test_scaled)

# Dibujar ROC para cada clase
plt.figure(figsize=(8,6))
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_bin[:, i], y_score[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Clase {i} (AUC={roc_auc:.2f})')

plt.plot([0,1], [0,1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Curvas ROC (Multiclase)')
plt.legend()
plt.show()

# Usar el modelo entrenado y los datos de prueba correctos
disp = ConfusionMatrixDisplay.from_estimator(
    model,
    X_test_scaled, # Usar datos de prueba ESCALADOS
    y_test,        # Usar etiquetas de prueba reales
    cmap=plt.cm.Blues,
    normalize=None
)

# Corregir el título a Multiclase
disp.ax_.set_title("Matriz de Confusión - Random Forest (Multiclase)")
plt.show()

# Generar predicciones de clase
y_pred_test = model.predict(X_test_scaled)

report_log = classification_report(y_test, y_pred_test)
print(report_log)

"""Como podemos observar Random Forest no llega a tener la precisión de la regresion logistica, teniendo también un problema sobre todo a la hora de clasificar la clase 1 y 2.

## 8. Tercer modelo: Gradient Boosting

Para mejorar la eficiencia de Gradient Boosting utilizaremos GridSearchCV para la búsqueda de los mejores hiperparámetros. De esta forma conseguimos aumentar el rendimiento
"""

# Probar diferentes hiperparámetros a mano
params = [
    {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3},
    {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 5},
    {'n_estimators': 300, 'learning_rate': 0.05, 'max_depth': 7}
]

best_acc = 0
best_params = None

for p in params:
    model = GradientBoostingClassifier(**p, random_state=42)
    model.fit(X_train, y_train)
    acc = model.score(X_test, y_test)

    print(p, "=> accuracy:", acc)

    if acc > best_acc:
        best_acc = acc
        best_params = p

print("Mejores hiperparámetros:", best_params)

"""Como hemos visto la mejor combinación es: 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200"""

# Número de folds que queremos probar
folds = [3, 5, 7, 10]

# Modelo Gradient Boosting
model = GradientBoostingClassifier(n_estimators=200,learning_rate=0.05,max_depth=5,random_state=42)

# Cross-validation para cada número de folds
for k in folds:
    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')

    print(f"{k} folds:")
    print(f"  Accuracy por fold: {scores}")
    print(f"  Accuracy media: {scores.mean():.4f}\n")

# Entrenamos modelo final sobre todo el set de entrenamiento
model.fit(X_train_scaled, y_train)
test_acc = model.score(X_test_scaled, y_test)

print("Accuracy final en test:", test_acc)

"""Como podemos observar, con 3 folds el modelo Gradient Boosting obtiene una precisión media de 86.6%, mientras que al aumentar a 5, 7 y 10 folds la precisión media mejora gradualmente, alcanzando aproximadamente 88.8–89.0%. La evaluación final en el conjunto de prueba muestra una precisión de 91.8%, lo que indica que el modelo generaliza correctamente y no presenta signos de sobreajuste.

Además, utilizar un número intermedio de folds, como 5 o 7, ofrece un buen equilibrio entre obtener una estimación estable de la accuracy y no incrementar demasiado el número de particiones del conjunto de entrenamiento.

Analizaremos este resultado calculando la recta ROC, PR y la matriz de confusion:
"""

# Convertimos y a formato binario (One-hot)
y_bin = label_binarize(y_test, classes=[0,1,2,3])
n_classes = y_bin.shape[1]

# Predicciones de probabilidades
y_score = model.predict_proba(X_test_scaled)  # lista de arrays por clase

# Para cada clase, dibujamos la curva Precision-Recall
plt.figure(figsize=(8,6))
for i in range(n_classes):
    precision, recall, _ = precision_recall_curve(y_bin[:, i], y_score[:, i])
    pr_auc = auc(recall, precision)
    plt.plot(recall, precision, label=f'Clase {i} (AUC={pr_auc:.2f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Curvas Precision-Recall (Multiclase)')
plt.legend()
plt.show()

# Binarizamos las clases
y_bin = label_binarize(y_test, classes=[0,1,2,3])
n_classes = y_bin.shape[1]

# Probabilidades predichas por el modelo
y_score = model.predict_proba(X_test_scaled)

# Dibujar ROC para cada clase
plt.figure(figsize=(8,6))
for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_bin[:, i], y_score[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Clase {i} (AUC={roc_auc:.2f})')

plt.plot([0,1], [0,1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Curvas ROC (Multiclase)')
plt.legend()
plt.show()

# Usar el modelo entrenado y los datos de prueba correctos
disp = ConfusionMatrixDisplay.from_estimator(
    model,
    X_test_scaled, # Usar datos de prueba ESCALADOS
    y_test,        # Usar etiquetas de prueba reales
    cmap=plt.cm.Blues,
    normalize=None
)

# Corregir el título a Multiclase
disp.ax_.set_title("Matriz de Confusión - Gradient Boosting (Multiclase)")
plt.show()

"""Como podemos observar Gradient Boosting consigue una precisión intermedia entre Random Forest y regresion logistica, teniendo también un problema sobre todo a la hora de clasificar la clase 1 y 2."""

# Generar predicciones de clase
y_pred_test = model.predict(X_test_scaled)

report_log = classification_report(y_test, y_pred_test)
print(report_log)

"""## 9. Análisis final

## Resultados:
"""

# Resultados (puedes sustituirlos si quieres los exactos de tu ejecución)
models = ["Logistic Regression", "Random Forest", "Gradient Boosting"]
train_acc = [0.928, 0.863, 0.886]
test_acc  = [0.947, 0.873, 0.918]

# Crear dataframe resumen
df_results = pd.DataFrame({
    "Modelo": models,
    "Train Accuracy": train_acc,
    "Test Accuracy": test_acc
})

print(df_results)

# --- GRÁFICO ---
x = np.arange(len(models))
width = 0.35  # ancho de las barras

plt.figure(figsize=(10, 6))

# Barras Train y Test
plt.bar(x - width/2, train_acc, width, label="Train Accuracy")
plt.bar(x + width/2, test_acc,  width, label="Test Accuracy")

plt.xticks(x, models, rotation=20)
plt.ylabel("Accuracy")
plt.title("Comparación de Accuracy por Modelo (Train vs Test)")
plt.legend()
plt.tight_layout()
plt.show()

"""### Evaluacion de modelos:

Tras entrenar los tres modelos de clasificación supervisada; Logistic Regression, Random Forest y Gradient Boosting. Para obtener el rendimiento, utilicé validación cruzada con 3, 5, 7 y 10 particiones, observándose cómo aumenta la estabilidad de la accuracy al incrementar el número de folds.

- Regresión Logística:
  - Accuracy media en validación cruzada: 92.8%
  - Accuracy en test: 94.7%
  - Explicación: La regresión logística proporciona un rendimiento sorprendentemente elevado dada su simplicidad. El modelo generaliza bien y no presenta síntomas de sobreajuste.

- Random Forest:
  - Accuracy media en validación cruzada: 86.3%
  - Accuracy en test: 87.3%
  - Explicación: Aunque su rendimiento es inferior al de la regresión logística, el modelo sigue mostrando buena capacidad de generalización.

- Gradient Boosting:
  - Accuracy media en validación cruzada: 88.6%
  - Accuracy en test: 91.0%
  - Explicación: El modelo mejora de forma notable respecto al Random Forest. Su arquitectura secuencial permite capturar relaciones más complejas entre las variables.

La Regresión Logística es el modelo óptimo, para la clasificación del precio de los móviles, con una precisión del 94.7%. Su superioridad se debe a que la RAM es la característica más determinante, y la relación entre RAM y el rango de precio es lineal o monótona. Esto permite que el modelo simple capture la esencia del problema de clasificación sin el riesgo de sobreajuste o la complejidad inherente de los modelos basados en árboles.
"""
